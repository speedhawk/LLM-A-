{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/speedhawk/LLM-A-/blob/main/LLM_Astar_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxK4ZneVyNSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ea047d-a9e6-4f69-badd-9ae9efbc1867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx9sm9ekySqk"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6p8a0wDyUJf"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IanF76AuyWzH",
        "outputId": "0911b871-4519-4837-d9d2-5c4d62c5fead"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import openai\n",
        "import copy\n",
        "import re\n",
        "import openpyxl\n",
        "import seaborn as sns\n",
        "import seaborn.objects as so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jQvmXOGwyajL",
        "outputId": "7155b87d-6060-4fe0-8142-377242072583"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Astar_LLM_ImgData():\n",
        "  def __init__(self, path, start, goal, env_discription, map_range, max_margin, map_sheet='Sheet1', dis_table_sheet='Sheet2') -> None:\n",
        "      self.file_path = path\n",
        "      self.map_sheet = map_sheet\n",
        "      self.dis_table_sheet = dis_table_sheet\n",
        "      self.map_range = map_range\n",
        "      self.max_margin = max_margin\n",
        "      self.map = None\n",
        "      self.dis_table = None\n",
        "      self.result_demo = np.ones((self.max_margin, self.max_margin, 3))\n",
        "      self.action_space = {0: [-1, 0],\n",
        "                           1: [1, 0],\n",
        "                           2: [0, 1],\n",
        "                           3: [0, -1],\n",
        "                           4: [-1, 1],\n",
        "                           5: [-1, -1],\n",
        "                           6: [1, 1],\n",
        "                           7: [1, -1]}\n",
        "\n",
        "      self.start_node = start\n",
        "      self.target_node = goal\n",
        "      self.cur_node = self.start_node\n",
        "      self.env_discription = env_discription\n",
        "      self.pre_node = None\n",
        "\n",
        "      self.step = 0\n",
        "      self.min_step = 0\n",
        "      self.min_dis = 1024\n",
        "      self.min_node = None\n",
        "\n",
        "      self.open_queue = {}   # component structure: {step: neighbour node}\n",
        "      self.close_queue = {str(self.start_node): self.start_node}  # structure: {'current node': parent node}\n",
        "      self.par_nodes = {}   # component structure: {step: current node}\n",
        "\n",
        "\n",
        "      self.model_name = \"gpt-3.5-turbo-16k\"\n",
        "\n",
        "      self.openai_key = 'YOUR_KEY'\n",
        "\n",
        "\n",
        "      self.ini_massages = None\n",
        "\n",
        "\n",
        "  def generate_map(self):\n",
        "    wb = openpyxl.load_workbook(self.file_path)\n",
        "    ws = wb[self.map_sheet]\n",
        "    _range = self.map_range\n",
        "    map = []\n",
        "    for row in ws[_range]:\n",
        "      map_row = []\n",
        "      for cell in row:\n",
        "        map_row.append(cell.value)\n",
        "      map.append(map_row)\n",
        "    map = np.array(map) # shape: [32, 32] or [16, 16]\n",
        "    self.map = map\n",
        "\n",
        "  def generate_dis_table(self):\n",
        "    wb = openpyxl.load_workbook(self.file_path)\n",
        "    ws = wb[self.dis_table_sheet]\n",
        "    _range = self.map_range\n",
        "    dis_table = []\n",
        "    for row in ws[_range]:\n",
        "      dis_table_row = []\n",
        "      for cell in row:\n",
        "        dis_table_row.append(cell.value)\n",
        "      dis_table.append(dis_table_row)\n",
        "    dis_table = np.array(dis_table) # shape: [32, 32] or [16, 16]\n",
        "    self.dis_table = dis_table\n",
        "\n",
        "  def message_initialize(self):\n",
        "    openai.api_key = self.openai_key\n",
        "    messages = [{'role': 'system', 'content': 'You are a good assistant! Man!'}, ]\n",
        "\n",
        "    # tell the basic information of the map\n",
        "    grid_world_inf = \"We have a \" + str(self.max_margin) + \"*\" + str(self.max_margin) + \" grid map in which there are totally \" + str(int(self.max_margin * self.max_margin)) + \" grids inside. In this map, we use [i, j] to represent the grid of ith row and jth column. Additionally, there is a binary number which is ‘1’ or ‘0’ in each grid to represent the obstacle conditions that ‘1’ means the grid is free whilst ‘0’ means the grid has been occupied by an obstacle and the agent cannot move to it. Our objective is to make an agent at the starting node to bypass obstacles and reach the designated target node. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. Here is the detail information below. \"\n",
        "    print(\"User: \", grid_world_inf)\n",
        "    # grid_world_inf = \"We have a 32*32 grid map in which there are totally 1024 grids inside. In this map, we use [i, j] to represent the grid of ith row and jth column. Additionally, there is a binary number which is ‘1’ or ‘0’ in each grid to represent the obstacle conditions that ‘1’ means the grid is free whilst ‘0’ means the grid has been occupied by an obstacle and the agent cannot move to it. Our objective is to make an agent at the starting node to bypass obstacles and reach the designated target node. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. Here is the detail information below. \"\n",
        "    # print(\"User: \", grid_world_inf)\n",
        "    messages.append({'role': 'user', 'content': grid_world_inf}, )\n",
        "    chat_for_task = openai.ChatCompletion.create(model=self.model_name, messages=messages)\n",
        "    reply_for_task = chat_for_task.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply_for_task}\")\n",
        "    messages.append({'role': 'assistant', 'content': reply_for_task})\n",
        "\n",
        "    # tell the start_node and target_node:\n",
        "    start_and_target = \"Start node: \" + str(self.start_node) + \"; Target node: \" + str(self.target_node) + \" which is at the agent's lowerleft direction. \"\n",
        "    print(\"User: \", start_and_target)\n",
        "    messages.append({'role': 'user', 'content': start_and_target}, )\n",
        "    chat_for_task = openai.ChatCompletion.create(model=self.model_name, messages=messages)\n",
        "    reply_for_task = chat_for_task.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply_for_task}\")\n",
        "    messages.append({'role': 'assistant', 'content': reply_for_task})\n",
        "\n",
        "    # tell the obstacle information.\n",
        "    obstacles_inf = self.env_discription\n",
        "    print(\"User: \", obstacles_inf)\n",
        "    messages.append({'role': 'user', 'content': obstacles_inf}, )\n",
        "    chat_for_task = openai.ChatCompletion.create(model=self.model_name, messages=messages)\n",
        "    reply_for_task = chat_for_task.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply_for_task}\")\n",
        "    messages.append({'role': 'assistant', 'content': reply_for_task})\n",
        "\n",
        "    # tell the agent action space and distance standard\n",
        "    action_inf = \"Agent action space (set current coordinate of the agent is [i, j]): Action 0: move up ([i, j] -> [i-1, j]); Action 1: move down ([i, j] -> [i+1, j]); Action 2: move right ([i, j] -> [i, j+1]); Action 3: move left ([i, j] -> [i, j-1]); Action 4: move upper right [i, j] -> [i-1, j+1] Action 5: move upper left ([i, j] -> [i-1, j-1]); Action 6: move lower right ([i, j] -> [i+1, j+1]); Action 7: move lower left ([i, j] -> [i+1, j-1]). Notice: 1. In this map, row index i increases from upward direction to downward direction, whilst column index j increases from leftward direction to righward direction. 2. When the agent is located in a grid adjacent to an obstacle, some actions in the action space will not be achieved. Please dynamically adjust the agent's actions at runtime. Distance standard: Euclidean distance. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt.\"\n",
        "    print(\"User: \", action_inf)\n",
        "    messages.append({'role': 'user', 'content': action_inf}, )\n",
        "    chat_for_task = openai.ChatCompletion.create(model=self.model_name, messages=messages)\n",
        "    reply_for_task = chat_for_task.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply_for_task}\")\n",
        "    messages.append({'role': 'assistant', 'content': reply_for_task})\n",
        "\n",
        "\n",
        "    # tell the objective\n",
        "    obj_inf = \"Our objective is to move the agent step by step with the actions above bypassing the obstacles from the start node to the target node. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt.\"\n",
        "    print(\"User: \", obj_inf)\n",
        "    messages.append({'role': 'user', 'content': obj_inf}, )\n",
        "    chat_for_task = openai.ChatCompletion.create(model=self.model_name, messages=messages)\n",
        "    reply_for_task = chat_for_task.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply_for_task}\")\n",
        "    messages.append({'role': 'assistant', 'content': reply_for_task})\n",
        "\n",
        "    self.ini_massages = copy.deepcopy(messages)\n",
        "\n",
        "  def extract_node(self, reply):\n",
        "    \"\"\"\n",
        "    This method is to extract the nodes of the search space in each step molocation. Commonly, there are multi-numbers\n",
        "    of coordinate tuples in the context. Therefore, it is fairly different from extract_node() function.\n",
        "    \"\"\"\n",
        "    context = reply\n",
        "    pattern = r'\\[[\\d,\\s]+\\]'\n",
        "    try:\n",
        "      lists = re.findall(pattern, context)\n",
        "      search_nodes = [eval(lst) for lst in lists]\n",
        "    except IndexError:\n",
        "      print('Empty list. It is caused by error output of LLM.')\n",
        "    finally:\n",
        "      if len(search_nodes) == 0:\n",
        "        return search_nodes\n",
        "      else:\n",
        "        return search_nodes[-1]\n",
        "\n",
        "  def opt_node_select(self, opt_actions):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    opt_actions: the action list return by function extract_node()\n",
        "    \"\"\"\n",
        "\n",
        "    opt_sup_node = self.cur_node\n",
        "    if sum(i>7 for i in opt_actions) != 0 or sum(i<0 for i in opt_actions) != 0 or \\\n",
        "    len(opt_actions) != len(set(opt_actions)) or len(opt_actions) == 0:   # error results\n",
        "      return\n",
        "    else:\n",
        "      for i in range(len(opt_actions)):                           # collect neighbour nodes into open.queue\n",
        "        act = opt_actions[i]\n",
        "        sup_x = self.cur_node[0] + self.action_space[act][0]\n",
        "        sup_y = self.cur_node[1] + self.action_space[act][1]\n",
        "        sup_node = [sup_x, sup_y]\n",
        "        if not(0 <= sup_x < self.max_margin) or not(0 <= sup_y < self.max_margin):        # move out of range\n",
        "          continue\n",
        "        elif abs(self.dis_table[sup_x][sup_y]) == 1024:           # move to obstacles\n",
        "          continue\n",
        "        elif str(sup_node) in list(self.close_queue.keys()):      # duplicated planing (meet the same node inside)\n",
        "          continue\n",
        "        elif sup_node in list(self.open_queue.values()):          # when duplicated searching, select the node with minimum G-cost as the parent node.\n",
        "\n",
        "          par_key = list(self.open_queue.keys())[list(self.open_queue.values()).index(sup_node)]\n",
        "          par = self.par_nodes[par_key]\n",
        "          g_par = abs(self.dis_table[self.start_node[0]][self.start_node[1]] - self.dis_table[par[0]][par[1]])\n",
        "          g_cur = abs(self.dis_table[self.start_node[0]][self.start_node[1]] - self.dis_table[self.cur_node[0]][self.cur_node[1]])\n",
        "          if g_cur < g_par:\n",
        "            sup_key = list(self.open_queue.keys())[list(self.open_queue.values()).index(sup_node)]\n",
        "            ori_par = self.par_nodes[sup_key]\n",
        "            self.par_nodes[sup_key] = self.cur_node\n",
        "\n",
        "        else: # add new nodes\n",
        "          end = 0 if len(self.open_queue) == 0 else list(self.open_queue.keys())[-1] + 1\n",
        "          self.open_queue[end] = sup_node\n",
        "          self.par_nodes[end] = self.cur_node\n",
        "\n",
        "      # select optimal node based on f-cost\n",
        "\n",
        "      opt_key = min(self.open_queue, key = lambda i: abs(self.dis_table[self.start_node[0]][self.start_node[1]] - \\\n",
        "                                                         self.dis_table[self.open_queue[i][0]][self.open_queue[i][1]]) + \\\n",
        "                    abs(self.dis_table[self.open_queue[i][0]][self.open_queue[i][1]]))\n",
        "\n",
        "      print(f'opt_key: {opt_key}')\n",
        "\n",
        "      to_close = self.open_queue.pop(opt_key)\n",
        "      to_close_par = self.par_nodes.pop(opt_key)\n",
        "\n",
        "      # update parent node\n",
        "\n",
        "      self.close_queue[str(to_close)] = to_close_par\n",
        "\n",
        "      # update current node of agent\n",
        "      self.pre_node = to_close_par\n",
        "      self.cur_node = to_close"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are totally three different environments with corresponding three sizes. Please select one and run it to load.**"
      ],
      "metadata": {
        "id": "Qds2Q2REM-9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_name = 'Aisle'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [1, 30]\n",
        "goal = [29, 2]\n",
        "map_range = 'A1:AF32'\n",
        "max_margin = 32\n",
        "env_discription = \"Obstacles: This map mainly includes two obstacles respectively located at region A and B. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:22], j: [13:18]; Region B: i: [26:31], j: [13:18]. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. \"\n",
        "\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iPRKo0quA3Vn",
        "outputId": "b1ac40ac-0638-4e4b-99bc-02c137f7167b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_name = 'Canyon'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [3, 23]\n",
        "goal = [21, 1]\n",
        "map_range = 'A1:AF32'\n",
        "max_margin = 32\n",
        "env_discription = \"Obstacles: This map mainly includes four obstacles respectively located at region A, B, C and D. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:15], j: [0:21]; Region B: i: [0:15], j: [25:31]; C: i: [22:31], j: [0:6]; Region D: i: [22:31], j: [10:31]. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. \"\n",
        "\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YS3TyFy1dIxZ",
        "outputId": "88a8a1a2-81c2-47d2-ef99-8bad1991eea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_name = 'Double_door'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [1, 29]\n",
        "goal = [28, 1]\n",
        "map_range = 'A1:AF32'\n",
        "max_margin = 32\n",
        "env_discription = \"Obstacles: This map includes four obstacles respectively located at region A, B, C, and D. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:26], j: [0:10]; Region B: i: [0:20], j: [11:21]; Region C: i: [30:31], j: [10:21]; Region D: i: [23:31], j: [21:31] except the grid i=23 and j=21. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. \"\n",
        "\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aWc9eJGXAwAh",
        "outputId": "0de2241e-0408-4997-fe49-df812abab239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "b32OCsqtS1bU",
        "outputId": "e05cbe53-764e-4f49-e864-2219241c7a79"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "map_name = 'Aisle_24'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [1, 22]\n",
        "goal = [21, 2]\n",
        "map_range = 'A1:X24'\n",
        "max_margin = 24\n",
        "env_discription = \"Obstacles: This map mainly includes two obstacles respectively located at region A and B. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:15], j: [9:13]; Region B: i: [19:23], j: [9:13]. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt.\"\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "F6-Z4QiIS1L4",
        "outputId": "b7e50b4e-13f1-4e5e-87b7-03352fd2f506"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "map_name = 'Canyon_24'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [2, 17]\n",
        "goal = [15, 2]\n",
        "map_range = 'A1:X24'\n",
        "max_margin = 24\n",
        "env_discription = \"Obstacles: This map mainly includes four obstacles respectively located at region A, B, C and D. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:11], j: [0:15]; Region B: i: [0:11], j: [19:23]; C: i: [16:23], j: [0:4]; Region D: i: [16:23], j: [8:23]. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. \"\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "k2dWqHmPS0ua",
        "outputId": "4b823d95-eeac-49dd-c747-b5e5f57cebeb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "map_name = 'Double_door_24'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [1, 22]\n",
        "goal = [19, 1]\n",
        "map_range = 'A1:X24'\n",
        "max_margin = 24\n",
        "env_discription = \"Obstacles: This map includes four obstacles respectively located at region A, B, C, and D. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:17], j: [0:7]; Region B: i: [0:15], j: [8:16]; Region C: i: [21:23], j: [7:15]; Region D: i: [18:23], j: [16:23] except the region where i=18 and j=16. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. \"\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d2-HwDzjyc5v",
        "outputId": "b000e886-17cf-4715-ac41-58d7e0571f05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "map_name = 'Aisle_16'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [1, 13]\n",
        "goal = [14, 1]\n",
        "env_discription = \"Obstacles: This map mainly includes two obstacles respectively located at region A and B. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:8], j: [7:9]; Region B: i: [12:15], j: [7:9]. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt.\"\n",
        "map_range ='A1:P16'\n",
        "max_margin = 16\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "eLgAw6OJygs4",
        "outputId": "67d669e4-def4-4038-af41-4919a7bcdb4e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "map_name = 'Canyon_16'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [2, 10]\n",
        "goal = [11, 1]\n",
        "env_discription = \"Obstacles: This map mainly includes four obstacles respectively located at region A, B, C and D. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:8], j: [0:9]; Region B: i: [0:8], j: [12:15]; C: i: [12:15], j: [0:3]; Region D: i: [12:15], j: [6:15]. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. \"\n",
        "map_range ='A1:P16'\n",
        "max_margin = 16\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DhS41qVoyika",
        "outputId": "476a5fd2-373c-4579-ed1d-f8632398b8fb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "map_name = 'Double_door_16'\n",
        "path = '/content/gdrive/MyDrive/LLM_Astar/maps/' + map_name + '.xlsx'\n",
        "start = [1, 15]\n",
        "goal = [14, 1]\n",
        "env_discription = \"Obstacles: This map includes four obstacles respectively located at region A, B, C, and D. This is the detail decription of grid regions occupied by obstacles, we deployed i and j as the index of row and column: Region A: i: [0:12], j: [0:4]; Region B: i: [0:10], j: [5:10]; Region C: i=15, j: [5:9]; Region D: i: [13:15], j: [10:15] except the grid i=13 and j=10. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt. \"\n",
        "map_range ='A1:P16'\n",
        "max_margin = 16\n",
        "env = Astar_LLM_ImgData(path, start, goal, env_discription, map_range, max_margin)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.generate_map()\n",
        "env.generate_dis_table()\n",
        "env.generate_dis_table()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3G4TVult_aoc",
        "outputId": "cb00be67-4ed4-4cbe-c4af-7c1e5c0d8e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1ErtQb0wyoVH",
        "outputId": "5aac4bdb-5283-46b3-c59a-c99e3c09c906"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Initialize language description of our map\n",
        "\n",
        "env.message_initialize()\n",
        "\n",
        "\n",
        "while env.cur_node != env.target_node:\n",
        "\n",
        "  messages = copy.deepcopy(env.ini_massages)\n",
        "\n",
        "  # ask for subset of action space\n",
        "\n",
        "  ask_opt_actions_inf = \"Now, the agent has moved from \" + str(env.pre_node) + \" to \" + str(env.cur_node) +\" which is the current position. Considering the current node the agent stays at above, based on the information of target nodes obstacles, action space and our objective, please for only the next step return me a subset of action space in which there are several optimal actions satisfied with the factors below: 1. Take care the obstacle regions I told you. Please always be careful do not move to them. 2. Take care the action space which includes at most 8 actions allowed, especially for some cells adjacent to the obstacles. 3. All of the optimal actions should serve for the purpose ‘achieve and reach the target node’. The agent must move in the correct direction while avoiding obstacles. Please prudentially deal with the causality relationship among obstacles and the correct direction and dinamically adjust and improve the solution. After selecting actions, please put the corresponding action numbers into a list and come out it. Considering the token limitation problem, please correctly and precisely remember the information above and do not return to much contexts for not only this but also the following prompt.\"\n",
        "  print(\"User: \", ask_opt_actions_inf)\n",
        "  messages.append({'role': 'user', 'content': ask_opt_actions_inf}, )\n",
        "  chat_for_task = openai.ChatCompletion.create(model=env.model_name, messages=messages)\n",
        "  reply_for_task = chat_for_task.choices[0].message.content\n",
        "  print(f\"ChatGPT: {reply_for_task}\")\n",
        "  messages.append({'role': 'assistant', 'content': reply_for_task})\n",
        "\n",
        "  ask_opt_actions_inf = \"Please come out ONLY the action number list ITSELF again with the format 'opt_actions: [1, 2, 3]' and WITHOUT ANY OTHER CONTENTS WITH LIST FORMAT. This is exclusively for the convenience for extracting the action number list. So please do not output redundant information with the list format. \"\n",
        "  print(\"User: \", ask_opt_actions_inf)\n",
        "  messages.append({'role': 'user', 'content': ask_opt_actions_inf}, )\n",
        "  chat_for_task = openai.ChatCompletion.create(model=env.model_name, messages=messages)\n",
        "  reply_for_task = chat_for_task.choices[0].message.content\n",
        "  print(f\"ChatGPT: {reply_for_task}\")\n",
        "  messages.append({'role': 'assistant', 'content': reply_for_task})\n",
        "\n",
        "  # extract optimal action list & save in search nodes memory\n",
        "\n",
        "  opt_actions = env.extract_node(reply_for_task)\n",
        "  # select the optimal next step and save in optimal path memory\n",
        "\n",
        "  env.opt_node_select(opt_actions)\n",
        "  # update step"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The code below is set to generate the result image.**"
      ],
      "metadata": {
        "id": "sdv9_edGNflB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_love_path = copy.deepcopy(env.close_queue)\n",
        "my_love_search = copy.deepcopy(env.open_queue)\n",
        "steps = len(my_love_path)\n",
        "open_steps = len(my_love_search)\n",
        "total_step = steps + open_steps"
      ],
      "metadata": {
        "id": "yiML62bTFfuV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "24101050-03cb-4597-d8b8-98a332bf8ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DLrmZv_VyvPW",
        "outputId": "35b62122-736c-4196-efa6-27eb970b647d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "the_map = copy.deepcopy(env.map)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# beta: avg_steering_angle -> maximum number of deviation angle\n",
        "\n",
        "def generate_traj_map(map):\n",
        "\n",
        "    map1 = copy.deepcopy(map)\n",
        "    map2 = copy.deepcopy(map1)\n",
        "    map = np.dstack((map, map1, map2)).astype(float)\n",
        "    # int type causes the exception of decimal recognition.\n",
        "\n",
        "    # Color the searched space, composing the searched grid map\n",
        "\n",
        "    start_x = env.start_node[0]\n",
        "    start_y = env.start_node[1]\n",
        "    cur_x = start_x\n",
        "    cur_y = start_y\n",
        "\n",
        "    for key in my_love_search:\n",
        "      current_node = eval(key)\n",
        "      x = current_node[0]\n",
        "      y = current_node[1]\n",
        "      # print(f\"x: {x}\")\n",
        "      # print(f\"y: {y}\")\n",
        "      map[x][y][0] = 0.0\n",
        "      map[x][y][1] = 1.0\n",
        "      map[x][y][2] = 0.0\n",
        "\n",
        "    for key in my_love_path:\n",
        "      x = my_love_path[key][0]\n",
        "      y = my_love_path[key][1]\n",
        "      map[x][y][0] = 0.0\n",
        "      map[x][y][1] = 1.0\n",
        "      map[x][y][2] = 0.0\n",
        "\n",
        "    cur = goal\n",
        "    # cur = env.cur_node\n",
        "    parent = my_love_path[str(cur)]\n",
        "    x_axis = [goal[0]]\n",
        "    y_axis = [goal[1]]\n",
        "    bas_dir_vector = np.array([0.0, 1.0])\n",
        "    dis_vector = np.array(goal)-np.array(start)\n",
        "    sum_angle = 0.0\n",
        "    avg_angle = 0.0\n",
        "    count = 0\n",
        "    max_times = 0\n",
        "\n",
        "    # trace back the path, calculating the length and avarage curvature\n",
        "\n",
        "    while(parent != start):\n",
        "      count += 1\n",
        "\n",
        "      # update\n",
        "      x_axis.append(parent[0])\n",
        "      y_axis.append(parent[1])\n",
        "      my_love_path.pop(str(cur))\n",
        "\n",
        "      # caluculate angles\n",
        "\n",
        "      dir_vector = np.array(cur) - np.array(parent)\n",
        "      # print(f\"bas_dir_vector: {bas_dir_vector}\")\n",
        "\n",
        "      # steering angle toward last vector\n",
        "      angle = abs(np.arccos(np.clip(np.dot(dir_vector, bas_dir_vector)/(np.linalg.norm(dir_vector)*np.linalg.norm(bas_dir_vector)), -1.0, 1.0)))\n",
        "\n",
        "      # steering angle toward displacement\n",
        "      dis_angle = abs(np.arccos(np.clip(np.dot(dir_vector, dis_vector)/(np.linalg.norm(dir_vector)*np.linalg.norm(dis_vector)), -1.0, 1.0)))\n",
        "\n",
        "      dis_angle = round(dis_angle, 3)\n",
        "      angle = round(angle, 3)\n",
        "      # print(f\"angle: {angle}\")\n",
        "\n",
        "      sum_angle += angle\n",
        "      avg_angle = round(sum_angle / float(count), 2)\n",
        "\n",
        "      if dis_angle > math.pi / 2.0:\n",
        "        max_times += 1\n",
        "\n",
        "      cur = parent\n",
        "      parent = my_love_path[str(cur)]\n",
        "      bas_dir_vector = dir_vector\n",
        "      # print(f\"bas_dir_vector: {bas_dir_vector}\\n\")\n",
        "\n",
        "    x_axis.append(env.start_node[0])\n",
        "    y_axis.append(env.start_node[1])\n",
        "\n",
        "    # return map, x_axis, y_axis, avg_angle\n",
        "    return map, x_axis, y_axis, avg_angle, max_times\n",
        "\n",
        "def color_path(map, x_axis, y_axis):\n",
        "\n",
        "    color_0 = 1.0\n",
        "    color_2 = 0.0\n",
        "    delta_color = 1.0 / float(len(x_axis))\n",
        "    for i in range(len(x_axis)):\n",
        "      x = x_axis[i]\n",
        "      y = y_axis[i]\n",
        "      map[x][y][0] = color_0\n",
        "      map[x][y][1] = 0.0\n",
        "      map[x][y][2] = color_2\n",
        "\n",
        "      color_0 -= delta_color\n",
        "      color_2 += delta_color\n",
        "\n",
        "    traj_map = (map * 255.0).astype(int)\n",
        "\n",
        "    return traj_map, len(x_axis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JBTypyHG5WRF",
        "outputId": "65865784-741f-4a95-9f50-34e2c7336925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cx4euMmyyywn",
        "outputId": "d05e0d55-646d-494a-e903-3c9c2760af29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "final_map, x_axis, y_axis, avg_angle, max_times = generate_traj_map(the_map)\n",
        "final_map, path_len = color_path(final_map, x_axis, y_axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuBG2FCny1Vb"
      },
      "outputs": [],
      "source": [
        "# Generate the interactive records\n",
        "fig, axes = plt.subplots(1, 1, figsize=[12, 4])\n",
        "axes.imshow(final_map)\n",
        "axes.axis('off')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM+P9LEJlo/pfR7AsrzH/xo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}